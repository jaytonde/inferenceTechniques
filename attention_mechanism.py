import math
import torch

def attention(q,k,v):
    d_k     = q.shape[-1]
    scores  = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(d_k)
    weights = torch.softmax(scores, dim=-1)
    return torch.matmul(weights, v)

def causal_attention(q,k,v):
    d_k    = q.shape[-1]
    scores = torch.matmul(q, k.transpose(-2,-1))/math.sqrt(d_k)
    mask   = torch.triu(
                            torch.ones(
                                scores.shape[-1],
                                scores.shape[-1]
                            ),
                            diagonal=1
                        )
    scores = scores.masked_fill(mask, float('-inf'))
    return torch.matmul(torch.softmax(scores, dim=-1), v)
